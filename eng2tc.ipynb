{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c41dec1-5295-43f5-8196-402036f5188b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time, random\n",
    "import h5py\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a781add-0afa-4203-a5b3-39ea4e96b612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import *\n",
    "from keras.initializers import *\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e93466b1-2260-4ee3-be5f-64aa57ba9b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training設定\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "latent_dim = 256\n",
    "num_samples = 10000\n",
    "\n",
    "data_path = \"tc.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0791560f-adfc-472d-966c-bce5cbc0f0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a783417-efd3-461e-907e-cc0ec9e10750",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, erase = line.split('\\t')\n",
    "    #用\"\\t\"作為SOS\n",
    "    #以\"\\n\"作為EOS\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df39237c-433b-4412-aee8-f2997acc72c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 73\n",
      "Number of unique output tokens: 2037\n",
      "Max sequence length for inputs: 26\n",
      "Max sequence length for outputs: 22\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bf84e3c-f20d-454e-b567-fb617f6f17be",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caca83b7-b217-4d39-8d30-f2629851b0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "#去掉encoders的輸出，只保留狀態h及c\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "#以encoders保留的h及c狀態進行初始化\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43e6d3d9-70c5-4af8-be05-0f7b96cbbab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, None, 73)]   0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, None, 2037)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 256),        337920      ['input_1[0][0]']                \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, None, 256),  2349056     ['input_2[0][0]',                \n",
      "                                 (None, 256),                     'lstm[0][1]',                   \n",
      "                                 (None, 256)]                     'lstm[0][2]']                   \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, None, 2037)   523509      ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,210,485\n",
      "Trainable params: 3,210,485\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "[<KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'lstm')>, <KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'lstm')>, <KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'lstm')>]\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "print(model.layers[-3].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99c51be0-72b3-444e-8abd-c1407f82a594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input_data shape: (10000, 26, 73)\n",
      "decoder_input_data shape: (10000, 22, 2037)\n",
      "decoder_target_data shape: (10000, 22, 2037)\n"
     ]
    }
   ],
   "source": [
    "print(\"encoder_input_data shape:\",encoder_input_data.shape)\n",
    "print(\"decoder_input_data shape:\",decoder_input_data.shape)\n",
    "print(\"decoder_target_data shape:\",decoder_target_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d904ff4b-b6e5-4498-a181-5cc57e4c12cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\anaconda20211117\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "125/125 [==============================] - 28s 207ms/step - loss: 1.8467 - val_loss: 2.1991\n",
      "Epoch 2/100\n",
      "125/125 [==============================] - 24s 196ms/step - loss: 1.7616 - val_loss: 2.1558\n",
      "Epoch 3/100\n",
      "125/125 [==============================] - 24s 190ms/step - loss: 1.7039 - val_loss: 2.0902\n",
      "Epoch 4/100\n",
      "125/125 [==============================] - 24s 194ms/step - loss: 1.6199 - val_loss: 2.0288\n",
      "Epoch 5/100\n",
      "125/125 [==============================] - 24s 194ms/step - loss: 1.5558 - val_loss: 1.9594\n",
      "Epoch 6/100\n",
      "125/125 [==============================] - 24s 192ms/step - loss: 1.4907 - val_loss: 1.9118\n",
      "Epoch 7/100\n",
      "125/125 [==============================] - 24s 192ms/step - loss: 1.4226 - val_loss: 1.8292\n",
      "Epoch 8/100\n",
      "125/125 [==============================] - 24s 196ms/step - loss: 1.3354 - val_loss: 1.7753\n",
      "Epoch 9/100\n",
      "125/125 [==============================] - 24s 190ms/step - loss: 1.2581 - val_loss: 1.6963\n",
      "Epoch 10/100\n",
      "125/125 [==============================] - 23s 186ms/step - loss: 1.1573 - val_loss: 1.6899\n",
      "Epoch 11/100\n",
      "125/125 [==============================] - 23s 184ms/step - loss: 1.1026 - val_loss: 1.6207\n",
      "Epoch 12/100\n",
      "125/125 [==============================] - 23s 186ms/step - loss: 1.0416 - val_loss: 1.5585\n",
      "Epoch 13/100\n",
      "125/125 [==============================] - 23s 186ms/step - loss: 0.9782 - val_loss: 1.5109\n",
      "Epoch 14/100\n",
      "125/125 [==============================] - 23s 187ms/step - loss: 0.9178 - val_loss: 1.4837\n",
      "Epoch 15/100\n",
      "125/125 [==============================] - 23s 185ms/step - loss: 0.8772 - val_loss: 1.4614\n",
      "Epoch 16/100\n",
      "125/125 [==============================] - 23s 184ms/step - loss: 0.8339 - val_loss: 1.4419\n",
      "Epoch 17/100\n",
      "125/125 [==============================] - 23s 184ms/step - loss: 0.8006 - val_loss: 1.4526\n",
      "Epoch 18/100\n",
      "125/125 [==============================] - 24s 192ms/step - loss: 0.7731 - val_loss: 1.4287\n",
      "Epoch 19/100\n",
      "125/125 [==============================] - 25s 197ms/step - loss: 0.7542 - val_loss: 1.4361\n",
      "Epoch 20/100\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 0.7248 - val_loss: 1.4426\n",
      "Epoch 21/100\n",
      "125/125 [==============================] - 24s 189ms/step - loss: 0.6992 - val_loss: 1.4235\n",
      "Epoch 22/100\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 0.6776 - val_loss: 1.4246\n",
      "Epoch 23/100\n",
      "125/125 [==============================] - 25s 204ms/step - loss: 0.6642 - val_loss: 1.4383\n",
      "Epoch 24/100\n",
      "125/125 [==============================] - 24s 193ms/step - loss: 0.6413 - val_loss: 1.4391\n",
      "Epoch 25/100\n",
      "125/125 [==============================] - 24s 195ms/step - loss: 0.6196 - val_loss: 1.4587\n",
      "Epoch 26/100\n",
      "125/125 [==============================] - 24s 192ms/step - loss: 0.6014 - val_loss: 1.4353\n",
      "Epoch 27/100\n",
      "125/125 [==============================] - 24s 193ms/step - loss: 0.5823 - val_loss: 1.4673\n",
      "Epoch 28/100\n",
      "125/125 [==============================] - 24s 192ms/step - loss: 0.5715 - val_loss: 1.4352\n",
      "Epoch 29/100\n",
      "125/125 [==============================] - 24s 196ms/step - loss: 0.7355 - val_loss: 1.4889\n",
      "Epoch 30/100\n",
      "125/125 [==============================] - 24s 190ms/step - loss: 0.6149 - val_loss: 1.4663\n",
      "Epoch 31/100\n",
      "125/125 [==============================] - 22s 175ms/step - loss: 0.5678 - val_loss: 1.4566\n",
      "Epoch 32/100\n",
      "125/125 [==============================] - 22s 172ms/step - loss: 0.5464 - val_loss: 1.4590\n",
      "Epoch 33/100\n",
      "125/125 [==============================] - 23s 184ms/step - loss: 0.5211 - val_loss: 1.4659\n",
      "Epoch 34/100\n",
      "125/125 [==============================] - 24s 193ms/step - loss: 0.5043 - val_loss: 1.4705\n",
      "Epoch 35/100\n",
      "125/125 [==============================] - 24s 196ms/step - loss: 0.4903 - val_loss: 1.4743\n",
      "Epoch 36/100\n",
      "125/125 [==============================] - 22s 179ms/step - loss: 0.4769 - val_loss: 1.4888\n",
      "Epoch 37/100\n",
      "125/125 [==============================] - 22s 176ms/step - loss: 0.4652 - val_loss: 1.4852\n",
      "Epoch 38/100\n",
      "125/125 [==============================] - 22s 173ms/step - loss: 0.4598 - val_loss: 1.4907\n",
      "Epoch 39/100\n",
      "125/125 [==============================] - 23s 188ms/step - loss: 0.4439 - val_loss: 1.4963\n",
      "Epoch 40/100\n",
      "125/125 [==============================] - 23s 187ms/step - loss: 0.4328 - val_loss: 1.5059\n",
      "Epoch 41/100\n",
      "125/125 [==============================] - 24s 194ms/step - loss: 0.4232 - val_loss: 1.5062\n",
      "Epoch 42/100\n",
      "125/125 [==============================] - 23s 188ms/step - loss: 0.4119 - val_loss: 1.5230\n",
      "Epoch 43/100\n",
      "125/125 [==============================] - 23s 186ms/step - loss: 0.4028 - val_loss: 1.5228\n",
      "Epoch 44/100\n",
      "125/125 [==============================] - 25s 197ms/step - loss: 0.3929 - val_loss: 1.5373\n",
      "Epoch 45/100\n",
      "125/125 [==============================] - 25s 198ms/step - loss: 0.3845 - val_loss: 1.5411\n",
      "Epoch 46/100\n",
      "125/125 [==============================] - 23s 180ms/step - loss: 0.3762 - val_loss: 1.5494\n",
      "Epoch 47/100\n",
      "125/125 [==============================] - 24s 193ms/step - loss: 0.3706 - val_loss: 1.5495\n",
      "Epoch 48/100\n",
      "125/125 [==============================] - 24s 189ms/step - loss: 0.3613 - val_loss: 1.5611\n",
      "Epoch 49/100\n",
      "125/125 [==============================] - 24s 189ms/step - loss: 0.3672 - val_loss: 1.5712\n",
      "Epoch 50/100\n",
      "125/125 [==============================] - 24s 190ms/step - loss: 0.3447 - val_loss: 1.5682\n",
      "Epoch 51/100\n",
      "125/125 [==============================] - 24s 190ms/step - loss: 0.3356 - val_loss: 1.5785\n",
      "Epoch 52/100\n",
      "125/125 [==============================] - 24s 190ms/step - loss: 0.3291 - val_loss: 1.5865\n",
      "Epoch 53/100\n",
      "125/125 [==============================] - 24s 188ms/step - loss: 0.3221 - val_loss: 1.5919\n",
      "Epoch 54/100\n",
      "125/125 [==============================] - 24s 190ms/step - loss: 0.3146 - val_loss: 1.6112\n",
      "Epoch 55/100\n",
      "125/125 [==============================] - 24s 191ms/step - loss: 0.3086 - val_loss: 1.6057\n",
      "Epoch 56/100\n",
      "125/125 [==============================] - 23s 186ms/step - loss: 0.3034 - val_loss: 1.6089\n",
      "Epoch 57/100\n",
      "125/125 [==============================] - 24s 190ms/step - loss: 0.2964 - val_loss: 1.6244\n",
      "Epoch 58/100\n",
      "125/125 [==============================] - 24s 190ms/step - loss: 0.2891 - val_loss: 1.6286\n",
      "Epoch 59/100\n",
      "125/125 [==============================] - 24s 190ms/step - loss: 0.3212 - val_loss: 1.6254\n",
      "Epoch 60/100\n",
      "125/125 [==============================] - 24s 190ms/step - loss: 0.3029 - val_loss: 1.6224\n",
      "Epoch 61/100\n",
      "125/125 [==============================] - 25s 197ms/step - loss: 0.2881 - val_loss: 1.6340\n",
      "Epoch 62/100\n",
      "125/125 [==============================] - 24s 192ms/step - loss: 0.2783 - val_loss: 1.6429\n",
      "Epoch 63/100\n",
      "125/125 [==============================] - 25s 196ms/step - loss: 0.2709 - val_loss: 1.6522\n",
      "Epoch 64/100\n",
      "125/125 [==============================] - 24s 190ms/step - loss: 0.2648 - val_loss: 1.6560\n",
      "Epoch 65/100\n",
      "125/125 [==============================] - 24s 190ms/step - loss: 0.2598 - val_loss: 1.6674\n",
      "Epoch 66/100\n",
      "125/125 [==============================] - 23s 188ms/step - loss: 0.2549 - val_loss: 1.6763\n",
      "Epoch 67/100\n",
      "125/125 [==============================] - 24s 188ms/step - loss: 0.2555 - val_loss: 1.6802\n",
      "Epoch 68/100\n",
      "125/125 [==============================] - 23s 188ms/step - loss: 0.2481 - val_loss: 1.6834\n",
      "Epoch 69/100\n",
      "125/125 [==============================] - 23s 188ms/step - loss: 0.2741 - val_loss: 1.6722\n",
      "Epoch 70/100\n",
      "125/125 [==============================] - 24s 191ms/step - loss: 0.2454 - val_loss: 1.6830\n",
      "Epoch 71/100\n",
      "125/125 [==============================] - 24s 189ms/step - loss: 0.2329 - val_loss: 1.6996\n",
      "Epoch 72/100\n",
      "125/125 [==============================] - 25s 197ms/step - loss: 0.2275 - val_loss: 1.7057\n",
      "Epoch 73/100\n",
      "125/125 [==============================] - 24s 193ms/step - loss: 0.2232 - val_loss: 1.7137\n",
      "Epoch 74/100\n",
      "125/125 [==============================] - 23s 188ms/step - loss: 0.2195 - val_loss: 1.7150\n",
      "Epoch 75/100\n",
      "125/125 [==============================] - 24s 189ms/step - loss: 0.2164 - val_loss: 1.7272\n",
      "Epoch 76/100\n",
      "125/125 [==============================] - 23s 187ms/step - loss: 0.2125 - val_loss: 1.7297\n",
      "Epoch 77/100\n",
      "125/125 [==============================] - 24s 190ms/step - loss: 0.2094 - val_loss: 1.7311\n",
      "Epoch 78/100\n",
      "125/125 [==============================] - 24s 190ms/step - loss: 0.2066 - val_loss: 1.7517\n",
      "Epoch 79/100\n",
      "125/125 [==============================] - 24s 190ms/step - loss: 0.2036 - val_loss: 1.7513\n",
      "Epoch 80/100\n",
      "125/125 [==============================] - 24s 191ms/step - loss: 0.1997 - val_loss: 1.7536\n",
      "Epoch 81/100\n",
      "125/125 [==============================] - 24s 190ms/step - loss: 0.1966 - val_loss: 1.7652\n",
      "Epoch 82/100\n",
      "125/125 [==============================] - 24s 190ms/step - loss: 0.1954 - val_loss: 1.7694\n",
      "Epoch 83/100\n",
      "125/125 [==============================] - 24s 191ms/step - loss: 0.1915 - val_loss: 1.7690\n",
      "Epoch 84/100\n",
      "125/125 [==============================] - 24s 189ms/step - loss: 0.1886 - val_loss: 1.7742\n",
      "Epoch 85/100\n",
      "125/125 [==============================] - 24s 189ms/step - loss: 0.1852 - val_loss: 1.7888\n",
      "Epoch 86/100\n",
      "125/125 [==============================] - 24s 190ms/step - loss: 0.1818 - val_loss: 1.7883\n",
      "Epoch 87/100\n",
      "125/125 [==============================] - 24s 195ms/step - loss: 0.1838 - val_loss: 1.8411\n",
      "Epoch 88/100\n",
      "125/125 [==============================] - 24s 191ms/step - loss: 0.2194 - val_loss: 1.7847\n",
      "Epoch 89/100\n",
      "125/125 [==============================] - 24s 191ms/step - loss: 0.1798 - val_loss: 1.7848\n",
      "Epoch 90/100\n",
      "125/125 [==============================] - 24s 191ms/step - loss: 0.1726 - val_loss: 1.7992\n",
      "Epoch 91/100\n",
      "125/125 [==============================] - 24s 189ms/step - loss: 0.1683 - val_loss: 1.8103\n",
      "Epoch 92/100\n",
      "125/125 [==============================] - 24s 194ms/step - loss: 0.1652 - val_loss: 1.8110\n",
      "Epoch 93/100\n",
      "125/125 [==============================] - 24s 192ms/step - loss: 0.1628 - val_loss: 1.8229\n",
      "Epoch 94/100\n",
      "125/125 [==============================] - 24s 189ms/step - loss: 0.1612 - val_loss: 1.8157\n",
      "Epoch 95/100\n",
      "125/125 [==============================] - 23s 188ms/step - loss: 0.1701 - val_loss: 1.8144\n",
      "Epoch 96/100\n",
      "125/125 [==============================] - 24s 189ms/step - loss: 0.1666 - val_loss: 1.8208\n",
      "Epoch 97/100\n",
      "125/125 [==============================] - 24s 189ms/step - loss: 0.1590 - val_loss: 1.8231\n",
      "Epoch 98/100\n",
      "125/125 [==============================] - 24s 191ms/step - loss: 0.1546 - val_loss: 1.8438\n",
      "Epoch 99/100\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 0.1561 - val_loss: 1.8395\n",
      "Epoch 100/100\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 0.1527 - val_loss: 1.8521\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27cf1a6d580>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training\n",
    "from keras.optimizers import * \n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, decay=0.001), loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "           batch_size=batch_size,\n",
    "           epochs=epochs, \n",
    "           validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5114fc4-374b-4c25-bc05-1b0f09bc3a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fbddccb-06ce-4f96-9507-5b950902ab7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    #以\\t當作SOS\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "        states_value = [h, c]\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ea841fc-5082-4969-892a-14d63de7575f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "輸入: Hi.\n",
      "輸出: 嗨！\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Hi.\n",
      "輸出: 嗨！\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Run.\n",
      "輸出: 你走跑。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Stop!\n",
      "輸出: 住手！\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Wait!\n",
      "輸出: 等一下！\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Wait!\n",
      "輸出: 等一下！\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Begin.\n",
      "輸出: 開始！\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Hello!\n",
      "輸出: 你好。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: I try.\n",
      "輸出: 我試試。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: I won!\n",
      "輸出: 我贏了。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Oh no!\n",
      "輸出: 不會吧。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Cheers!\n",
      "輸出: 乾杯!\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Got it?\n",
      "輸出: 你懂了嗎？\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Got it?\n",
      "輸出: 你懂了嗎？\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Got it?\n",
      "輸出: 你懂了嗎？\n",
      "\n",
      "--------------------------------------\n",
      "輸入: He ran.\n",
      "輸出: 他跑了。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Hop in.\n",
      "輸出: 靜吧！\n",
      "\n",
      "--------------------------------------\n",
      "輸入: I know.\n",
      "輸出: 我知道。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: I quit.\n",
      "輸出: 我不幹了。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: I quit.\n",
      "輸出: 我不幹了。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: I'm OK.\n",
      "輸出: 我沒事。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: I'm up.\n",
      "輸出: 我已經起來了。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Listen.\n",
      "輸出: 聽著。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: No way!\n",
      "輸出: 沒門！\n",
      "\n",
      "--------------------------------------\n",
      "輸入: No way!\n",
      "輸出: 沒門！\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Really?\n",
      "輸出: 你確定？\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Thanks!\n",
      "輸出: 謝謝！\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Try it.\n",
      "輸出: 試試吧。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: We try.\n",
      "輸出: 我們來試試。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Why me?\n",
      "輸出: 為什麼是我？\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Ask Tom.\n",
      "輸出: 回答熊。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Awesome!\n",
      "輸出: 好棒！\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Be calm.\n",
      "輸出: 冷靜點。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Be fair.\n",
      "輸出: 公平點。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Be kind.\n",
      "輸出: 友善點。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Be kind.\n",
      "輸出: 友善點。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Be nice.\n",
      "輸出: 友善點。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Be nice.\n",
      "輸出: 友善點。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Call me.\n",
      "輸出: 幫我一次。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Call us.\n",
      "輸出: 輕輕地移。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Come in.\n",
      "輸出: 進來。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Get Tom.\n",
      "輸出: 告訴我。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Get out!\n",
      "輸出: 滾出去。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Get out!\n",
      "輸出: 滾出去。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Go away!\n",
      "輸出: 走開！\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Go away!\n",
      "輸出: 走開！\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Go away.\n",
      "輸出: 走開！\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Go home.\n",
      "輸出: 回家。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Go home.\n",
      "輸出: 回家。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Goodbye!\n",
      "輸出: 再見！\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Goodbye!\n",
      "輸出: 再見！\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Hang on!\n",
      "輸出: 堅持。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Hang on!\n",
      "輸出: 堅持。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Hang on.\n",
      "輸出: 堅持。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: He came.\n",
      "輸出: 他來了。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: He runs.\n",
      "輸出: 他跑。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Help me.\n",
      "輸出: 幫我一下。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Help us.\n",
      "輸出: 幫幫我們。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Hit Tom.\n",
      "輸出: 它是傻瓜。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Hold on.\n",
      "輸出: 堅持。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Hug Tom.\n",
      "輸出: 請原諒湯姆。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Hug Tom.\n",
      "輸出: 請原諒湯姆。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: I agree.\n",
      "輸出: 我同意。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: I'm hot.\n",
      "輸出: 我覺得很熱。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: I'm ill.\n",
      "輸出: 我生病了。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: I'm sad.\n",
      "輸出: 我很難過。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: I'm shy.\n",
      "輸出: 我很害羞。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: I'm wet.\n",
      "輸出: 我濕了。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: It's OK.\n",
      "輸出: 沒關係。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: It's me.\n",
      "輸出: 是我。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Join us.\n",
      "輸出: 幫我一杯!\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Keep it.\n",
      "輸出: 留著吧。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Kiss me.\n",
      "輸出: 吻我。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Perfect!\n",
      "輸出: 完美！\n",
      "\n",
      "--------------------------------------\n",
      "輸入: See you.\n",
      "輸出: 再見！\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Shut up!\n",
      "輸出: 閉嘴！\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Skip it.\n",
      "輸出: 不管它。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Take it.\n",
      "輸出: 拿走吧。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Tell me.\n",
      "輸出: 告訴我！\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Tom won.\n",
      "輸出: 湯姆勝利了。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Wake up!\n",
      "輸出: 醒醒！\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Wash up.\n",
      "輸出: 試試試。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: We know.\n",
      "輸出: 我們知道。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Welcome.\n",
      "輸出: 歡迎。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Who won?\n",
      "輸出: 誰贏了？\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Why not?\n",
      "輸出: 為什麼不？\n",
      "\n",
      "--------------------------------------\n",
      "輸入: You run.\n",
      "輸出: 你跑。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: You win.\n",
      "輸出: 算你狠。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Back off!\n",
      "輸出: 往後退點。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Back off.\n",
      "輸出: 往後退點。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Be still.\n",
      "輸出: 靜靜的吧。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Beats me.\n",
      "輸出: 我一無積積。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Cuff him.\n",
      "輸出: 把他銬上。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Drive on.\n",
      "輸出: 往前開。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Get away!\n",
      "輸出: 滾！\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Get away!\n",
      "輸出: 滾！\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Get down!\n",
      "輸出: 趴下！\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Get lost!\n",
      "輸出: 滾。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Get lost!\n",
      "輸出: 滾。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Get lost.\n",
      "輸出: 滾。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(100):\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('--------------------------------------')\n",
    "    print('輸入:', input_texts[seq_index])\n",
    "    print('輸出:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9e50c738-46ac-46b9-9988-1fe510af080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path2 = \"test.txt\"\n",
    "input_texts2 = []\n",
    "#target_texts2 = []\n",
    "input_characters2 = set()\n",
    "#target_characters2 = set()\n",
    "with open(data_path2, 'r', encoding='utf-8') as f:\n",
    "    lines2 = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "99cb72cc-7df8-4c50-b711-d517fe935c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in lines2[: min(num_samples, len(lines2) - 1)]:\n",
    "    input_text2, target_text2, erase = line.split('\\t')\n",
    "    input_texts2.append(input_text2)\n",
    "    for char in input_text2:\n",
    "        if char not in input_characters2:\n",
    "            input_characters2.add(char)\n",
    "input_characters2 = sorted(list(input_characters2))\n",
    "num_encoder_tokens2 = len(input_characters2)\n",
    "max_encoder_seq_length2 = max([len(txt) for txt in input_texts2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b56b1f77-2e86-4b67-800b-2590e3ed8d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index2 = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters2)])\n",
    "encoder_input_data2 = np.zeros(\n",
    "    (len(input_texts2), max_encoder_seq_length2, num_encoder_tokens2),\n",
    "    dtype='float32')\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts2, target_texts2)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data2[i, t, input_token_index2[char]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "75fba6d7-482a-4d79-be1d-bd2cb89ad8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "輸入: I don't really think so.\n",
      "輸出: 我沒什麼積積。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Are you sure about that?\n",
      "輸出: 你還在那裡嗎？\n",
      "\n",
      "--------------------------------------\n",
      "輸入: I got fired from the company.\n",
      "輸出: 我把它扔了嗎?\n",
      "\n",
      "--------------------------------------\n",
      "輸入: I can't remember how to go there.\n",
      "輸出: 我聽到見到他們積了。\n",
      "\n",
      "--------------------------------------\n",
      "輸入: Wait!\n",
      "輸出: 等等！\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(5):\n",
    "    input_seq2 = encoder_input_data2[seq_index: seq_index + 1]\n",
    "    decoded_sentence2 = decode_sequence(input_seq2)\n",
    "    print('--------------------------------------')\n",
    "    print('輸入:', input_texts2[seq_index])\n",
    "    print('輸出:', decoded_sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa53a8a2-4f55-47c6-ad04-a98711c532ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
